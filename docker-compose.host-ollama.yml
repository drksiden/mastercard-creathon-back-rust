# Docker Compose для использования Ollama на хосте (не в контейнере)
services:
  api:
    build: .
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/payment_analytics
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OLLAMA_URL=http://host.docker.internal:11434  # Ollama на хосте
      - OLLAMA_MODEL=${OLLAMA_MODEL:-mixtral:8x7b-instruct}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-1.5-flash}
      - RUST_LOG=${RUST_LOG:-info}
      - HOST=0.0.0.0
      - PORT=3000
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Для доступа к Ollama на хосте
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=payment_analytics
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:

